{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "q_QuctboP3Tv",
        "W9TmdV-NT7vP"
      ],
      "gpuType": "A100",
      "mount_file_id": "1pQBkM-fUmwWgoDcKxDF_Zl82BHqdmloO",
      "authorship_tag": "ABX9TyNhe/lJTa2mdHUd3ba7qTl7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adinsegall/A-Simple-Synth/blob/main/DL4AM_Coursework_Segall.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DL4AM Coursework: Combining Loop Candidacy Detection and Source Separation Models\n",
        "\n",
        "Author: Adin Segall\n",
        "\n",
        "This notebook contains the code, results, and analysis for the DL4AM coursework project focused on applying deep learning techniques to musical audio processing tasks. The goal is to implement and combine two distinct models:\n",
        "\n",
        "1. A BiLSTM-based binary classifier to predict the \"loopability\" of short music segments (i.e. their suitability for seamless repetition). This model will learn on mel spectrogram input representations.  \n",
        "\n",
        "2. A convolutional variational autoencoder (CVAE) to perform source separation on mixed audio signals, isolating individual instrumental elements. The CVAE will operate on magnitude spectrogram inputs and produce time-frequency masks to extract sources.\n",
        "\n",
        "Both models will be trained and evaluated independently on the Free Music Archive (FMA) dataset, which provides source audio files, and optionally the Groove MIDI dataset for more granular rhythm pattern analysis. Key evaluation metrics will include accuracy and F1-score for the loop classifier, and signal-to-distortion ratio (SDR) and related source separation metrics for the CVAE.  \n",
        "\n",
        "After developing the standalone models, different strategies for joint optimization and transfer learning will be explored to assess potential performance gains from shared feature extraction and end-to-end training.\n",
        "\n",
        "The notebook is organized as follows:\n",
        "1. Setup\n",
        "   1.1 Imports\n",
        "   1.2 Logging\n",
        "   1.3 Device Configuration\n",
        "2. Dataset Loading and Preprocessing\n",
        "   2.1 FMA Dataset\n",
        "   2.2 Groove MIDI Dataset\n",
        "3. Model Implementations\n",
        "4. Training Procedures\n",
        "5. Evaluation and Results\n",
        "6. Model Combination Strategies\n",
        "7. Discussion and Interpretation\n",
        "8. Conclusion and Future Directions\n",
        "\n",
        "Code and analysis will be presented with the goal of academic rigor, suitable for conference submission in applied machine learning for audio. All key modeling choices and training details will be justified according to best practices established in the DL4AM module and relevant literature."
      ],
      "metadata": {
        "id": "cHUmgcxwOzwa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Setup"
      ],
      "metadata": {
        "id": "q_QuctboP3Tv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pretty_midi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTzxdza3up-G",
        "outputId": "8302af3b-04ca-449e-cdbb-acc9b7bbc381"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pretty_midi\n",
            "  Downloading pretty_midi-0.2.10.tar.gz (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from pretty_midi) (2.0.2)\n",
            "Collecting mido>=1.1.16 (from pretty_midi)\n",
            "  Downloading mido-1.3.3-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from pretty_midi) (1.17.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from mido>=1.1.16->pretty_midi) (24.2)\n",
            "Downloading mido-1.3.3-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pretty_midi\n",
            "  Building wheel for pretty_midi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretty_midi: filename=pretty_midi-0.2.10-py3-none-any.whl size=5592286 sha256=5c4645f28014bef6f167ddfd2e660e178daaf8652265024c062301eb29213608\n",
            "  Stored in directory: /root/.cache/pip/wheels/e6/95/ac/15ceaeb2823b04d8e638fd1495357adb8d26c00ccac9d7782e\n",
            "Successfully built pretty_midi\n",
            "Installing collected packages: mido, pretty_midi\n",
            "Successfully installed mido-1.3.3 pretty_midi-0.2.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2IGcnMKbwUfr",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title 1.1 Imports\n",
        "\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import logging\n",
        "import zipfile\n",
        "from os import listdir\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "import torchaudio\n",
        "from torchaudio.transforms import MelSpectrogram\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "import librosa\n",
        "import librosa.display\n",
        "import pretty_midi\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1.2 Logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "vWd4SCxsP-a_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1.3 Device Configuration\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "mMknV9GaTzBC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Datasets"
      ],
      "metadata": {
        "id": "W9TmdV-NT7vP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2.0 Dataset Download and Setup\n",
        "\n",
        "# Path to dataset storage inside Google Drive\n",
        "drive_root = '/content/drive/MyDrive/DL4AM_datasets'\n",
        "\n",
        "# Make sure the directory exists\n",
        "os.makedirs(drive_root, exist_ok=True)\n",
        "\n",
        "# Paths inside Google Drive\n",
        "fma_small_zip = os.path.join(drive_root, 'fma_small.zip')\n",
        "fma_small_folder = os.path.join(drive_root, 'fma_small')\n",
        "fma_small_url = 'https://os.unil.cloud.switch.ch/fma/fma_small.zip'\n",
        "\n",
        "fma_metadata_zip = os.path.join(drive_root, 'fma_metadata.zip')\n",
        "fma_metadata_folder = os.path.join(drive_root, 'fma_metadata')\n",
        "fma_metadata_url = 'https://os.unil.cloud.switch.ch/fma/fma_metadata.zip'\n",
        "\n",
        "egmd_zip = os.path.join(drive_root, 'e-gmd-v1.0.0-midi.zip')\n",
        "egmd_folder = os.path.join(drive_root, 'e-gmd-v1.0.0-midi')\n",
        "egmd_url = 'https://storage.googleapis.com/magentadata/datasets/e-gmd/v1.0.0/e-gmd-v1.0.0-midi.zip'\n",
        "\n",
        "# --- FMA Small ---\n",
        "if not os.path.exists(fma_small_folder):\n",
        "    print(f\"Downloading FMA Small to {fma_small_zip}...\")\n",
        "    !wget -O \"$fma_small_zip\" \"$fma_small_url\"\n",
        "\n",
        "    print(f\"Extracting to {fma_small_folder}...\")\n",
        "    !unzip -q \"$fma_small_zip\" -d \"$drive_root\"\n",
        "    os.remove(fma_small_zip)\n",
        "else:\n",
        "    print(f\"FMA Small already exists at {fma_small_folder}\")\n",
        "\n",
        "# --- FMA Metadata ---\n",
        "if not os.path.exists(fma_metadata_folder):\n",
        "    print(f\"Downloading FMA Metadata to {fma_metadata_zip}...\")\n",
        "    !wget -O \"$fma_metadata_zip\" \"$fma_metadata_url\"\n",
        "\n",
        "    print(f\"Extracting to {fma_metadata_folder}...\")\n",
        "    !unzip -q \"$fma_metadata_zip\" -d \"$drive_root\"\n",
        "    os.remove(fma_metadata_zip)\n",
        "else:\n",
        "    print(f\"FMA Metadata already exists at {fma_metadata_folder}\")\n",
        "\n",
        "# --- E-GMD MIDI ---\n",
        "if not os.path.exists(egmd_folder):\n",
        "    print(f\"Downloading E-GMD MIDI to {egmd_zip}...\")\n",
        "    !wget -O \"$egmd_zip\" \"$egmd_url\"\n",
        "\n",
        "    print(f\"Extracting to {egmd_folder}...\")\n",
        "    with zipfile.ZipFile(egmd_zip, 'r') as zip_ref:\n",
        "        zip_ref.extractall(drive_root)\n",
        "    os.remove(egmd_zip)\n",
        "else:\n",
        "    print(f\"E-GMD MIDI already exists at {egmd_folder}\")\n",
        "\n",
        "print(\"Dataset setup complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "89ZC2HpShKlc",
        "outputId": "39ba5b00-9a30-42de-beb5-a6f3f60296d9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FMA Small already exists at /content/drive/MyDrive/DL4AM_datasets/fma_small\n",
            "FMA Metadata already exists at /content/drive/MyDrive/DL4AM_datasets/fma_metadata\n",
            "E-GMD MIDI already exists at /content/drive/MyDrive/DL4AM_datasets/e-gmd-v1.0.0-midi\n",
            "Dataset setup complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2.1 FMA Dataset\n",
        "\n",
        "class FMADataset(Dataset):\n",
        "    \"\"\"Free Music Archive audio dataset (log-mel spectrograms + genre labels).\"\"\"\n",
        "    def __init__(self, root_dir, metadata_file, transform=None,\n",
        "                 n_mels=128, hop_length=512, sample_rate=22050, n_frames=860):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (str): Path to FMA root directory (e.g., 'fma_small').\n",
        "            metadata_file (str): Path to metadata CSV file (e.g., 'tracks.csv').\n",
        "            transform (callable, optional): Optional transform to apply to each sample.\n",
        "            n_mels (int): Number of mel bins.\n",
        "            hop_length (int): Hop length for mel spectrogram.\n",
        "            sample_rate (int): Target sample rate.\n",
        "            n_frames (int): Target number of spectrogram frames (for trimming/padding).\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.n_mels = n_mels\n",
        "        self.hop_length = hop_length\n",
        "        self.sample_rate = sample_rate\n",
        "        self.n_frames = n_frames\n",
        "\n",
        "        # Load metadata and filter valid tracks\n",
        "        logger.info(f\"Loading metadata from {metadata_file}\")\n",
        "        self.tracks = self._load_metadata(metadata_file)\n",
        "        logger.info(f\"Loaded metadata for {len(self.tracks)} tracks\")\n",
        "\n",
        "        # Find all MP3 files recursively in the root directory\n",
        "        self.file_paths = self._find_mp3_files(root_dir)\n",
        "        logger.info(f\"Found {len(self.file_paths)} MP3 files in {root_dir}\")\n",
        "\n",
        "        # Match files to metadata and keep only valid tracks\n",
        "        self._match_files_to_metadata()\n",
        "        logger.info(f\"Dataset contains {len(self.valid_indices)} valid tracks with metadata\")\n",
        "\n",
        "        # Create label encoder for genres\n",
        "        self._create_label_encoder()\n",
        "\n",
        "    def _load_metadata(self, metadata_file):\n",
        "        \"\"\"Load and parse the tracks metadata file\"\"\"\n",
        "        metadata = pd.read_csv(metadata_file, index_col=0, header=[0, 1])\n",
        "\n",
        "        # Keep only the tracks with a valid genre\n",
        "        metadata = metadata[metadata['track', 'genre_top'].notna()]\n",
        "\n",
        "        # Convert multi-level columns to single level for easier access\n",
        "        metadata.columns = [f\"{col[0]}_{col[1]}\" for col in metadata.columns]\n",
        "\n",
        "        return metadata\n",
        "\n",
        "    def _find_mp3_files(self, root_dir):\n",
        "        \"\"\"Recursively find all MP3 files in the root directory\"\"\"\n",
        "        file_paths = []\n",
        "        for root, dirs, files in os.walk(root_dir):\n",
        "            for file in files:\n",
        "                if file.endswith('.mp3'):\n",
        "                    file_paths.append(os.path.join(root, file))\n",
        "        return file_paths\n",
        "\n",
        "    def _match_files_to_metadata(self):\n",
        "        \"\"\"Match MP3 files to metadata entries\"\"\"\n",
        "        self.valid_indices = []\n",
        "        self.track_ids = []\n",
        "        self.valid_file_paths = []\n",
        "\n",
        "        for i, file_path in enumerate(self.file_paths):\n",
        "            # Extract track ID from filename (remove .mp3 extension)\n",
        "            filename = os.path.basename(file_path)\n",
        "            try:\n",
        "                track_id = int(filename.split('.')[0])\n",
        "\n",
        "                # Check if track exists in metadata\n",
        "                if track_id in self.tracks.index:\n",
        "                    self.valid_indices.append(i)\n",
        "                    self.track_ids.append(track_id)\n",
        "                    self.valid_file_paths.append(file_path)\n",
        "            except:\n",
        "                # Skip files with invalid names\n",
        "                continue\n",
        "\n",
        "    def _create_label_encoder(self):\n",
        "        \"\"\"Create a label encoder for genre labels\"\"\"\n",
        "        genres = [self.tracks.loc[track_id, 'track_genre_top'] for track_id in self.track_ids]\n",
        "        self.encoder = LabelEncoder()\n",
        "        self.encoder.fit(genres)\n",
        "        self.num_classes = len(self.encoder.classes_)\n",
        "        logger.info(f\"Found {self.num_classes} unique genre classes\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.valid_indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get file path and track ID\n",
        "        file_path = self.valid_file_paths[idx]\n",
        "        track_id = self.track_ids[idx]\n",
        "\n",
        "        try:\n",
        "            # Load audio file\n",
        "            waveform, sr = torchaudio.load(file_path)\n",
        "\n",
        "            # Convert to mono if stereo\n",
        "            if waveform.shape[0] > 1:\n",
        "                waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "            # Resample if necessary\n",
        "            if sr != self.sample_rate:\n",
        "                waveform = torchaudio.functional.resample(waveform, sr, self.sample_rate)\n",
        "\n",
        "            # Convert to numpy for librosa processing\n",
        "            waveform_np = waveform.numpy().squeeze()\n",
        "\n",
        "            # Compute mel spectrogram\n",
        "            mel_spec = librosa.feature.melspectrogram(\n",
        "                y=waveform_np,\n",
        "                sr=self.sample_rate,\n",
        "                n_mels=self.n_mels,\n",
        "                hop_length=self.hop_length\n",
        "            )\n",
        "            log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "\n",
        "            # Normalize to zero mean, unit variance\n",
        "            log_mel_spec = (log_mel_spec - log_mel_spec.mean()) / (log_mel_spec.std() + 1e-9)\n",
        "\n",
        "            # Pad or trim to fixed frame length\n",
        "            if log_mel_spec.shape[1] < self.n_frames:\n",
        "                pad_width = self.n_frames - log_mel_spec.shape[1]\n",
        "                log_mel_spec = np.pad(log_mel_spec, ((0, 0), (0, pad_width)), mode='constant')\n",
        "            else:\n",
        "                log_mel_spec = log_mel_spec[:, :self.n_frames]\n",
        "\n",
        "            # Add channel dimension\n",
        "            log_mel_spec = log_mel_spec[np.newaxis, :, :]\n",
        "\n",
        "            # Get genre label\n",
        "            genre = self.tracks.loc[track_id, 'track_genre_top']\n",
        "            label = self.encoder.transform([genre])[0]\n",
        "\n",
        "            sample = {\n",
        "                'spectrogram': torch.tensor(log_mel_spec, dtype=torch.float32),\n",
        "                'class_label': label,\n",
        "                'track_id': track_id\n",
        "            }\n",
        "\n",
        "            if self.transform:\n",
        "                sample = self.transform(sample)\n",
        "\n",
        "            return sample\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error processing {file_path}: {e}\")\n",
        "            # Return a dummy sample as fallback (you might want to handle this differently)\n",
        "            return {\n",
        "                'spectrogram': torch.zeros((1, self.n_mels, self.n_frames), dtype=torch.float32),\n",
        "                'class_label': 0,\n",
        "                'track_id': track_id\n",
        "            }\n",
        "\n",
        "    def get_train_val_indices(self, val_size=0.2, random_state=42):\n",
        "        \"\"\"Create random train/validation split indices\"\"\"\n",
        "        indices = list(range(len(self.valid_indices)))\n",
        "        n_val = int(len(indices) * val_size)\n",
        "\n",
        "        # Shuffle indices\n",
        "        np.random.seed(random_state)\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "        # Split indices\n",
        "        train_indices = indices[n_val:]\n",
        "        val_indices = indices[:n_val]\n",
        "\n",
        "        return train_indices, val_indices"
      ],
      "metadata": {
        "id": "lwAHkBd-T3nx",
        "cellView": "form"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2.2 Groove MIDI Dataset\n",
        "\n",
        "class GrooveMIDIDataset(Dataset):\n",
        "    def __init__(self, root_dir, max_files=50, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.max_files = max_files\n",
        "\n",
        "        # Get list of MIDI files\n",
        "        self.midi_files = [os.path.join(root_dir, 'midi', f) for f in os.listdir(os.path.join(root_dir, 'midi'))]\n",
        "\n",
        "        # Randomly select a subset of MIDI files\n",
        "        if len(self.midi_files) > max_files:\n",
        "            self.midi_files = random.sample(self.midi_files, max_files)\n",
        "\n",
        "        logger.info(f'Loaded {len(self.midi_files)} MIDI files from {root_dir}')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.midi_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        midi_path = self.midi_files[idx]\n",
        "\n",
        "        try:\n",
        "            # Load MIDI file\n",
        "            midi_data = pretty_midi.PrettyMIDI(midi_path)\n",
        "\n",
        "            # Extract features\n",
        "            onsets = []\n",
        "            velocities = []\n",
        "            durations = []\n",
        "            microtimings = []\n",
        "\n",
        "            for instrument in midi_data.instruments:\n",
        "                if not instrument.is_drum:\n",
        "                    for note in instrument.notes:\n",
        "                        onsets.append(midi_data.time_to_tick(note.start) / midi_data.resolution)\n",
        "                        velocities.append(note.velocity)\n",
        "                        durations.append(midi_data.time_to_tick(note.end - note.start) / midi_data.resolution)\n",
        "                        microtimings.append((note.start - midi_data.tick_to_time(midi_data.time_to_tick(note.start))) * midi_data.resolution)\n",
        "\n",
        "            # Create feature matrix\n",
        "            n_frames = 128  # Adjust as needed\n",
        "            onset_frame = np.zeros((n_frames,))\n",
        "            velocity_frame = np.zeros((n_frames,))\n",
        "            duration_frame = np.zeros((n_frames,))\n",
        "            microtiming_frame = np.zeros((n_frames,))\n",
        "\n",
        "            for onset, velocity, duration, microtiming in zip(onsets, velocities, durations, microtimings):\n",
        "                onset_idx = int(onset * (n_frames - 1))\n",
        "                onset_frame[onset_idx] = 1\n",
        "                velocity_frame[onset_idx] = velocity\n",
        "                duration_frame[onset_idx] = duration\n",
        "                microtiming_frame[onset_idx] = microtiming\n",
        "\n",
        "            # Stack features into a matrix\n",
        "            feature_matrix = np.stack((onset_frame, velocity_frame, duration_frame, microtiming_frame))\n",
        "\n",
        "            # Normalize feature matrix\n",
        "            mean = feature_matrix.mean()\n",
        "            std = feature_matrix.std()\n",
        "            feature_matrix = (feature_matrix - mean) / std\n",
        "\n",
        "            # Add channel dimension\n",
        "            feature_matrix = feature_matrix[np.newaxis, :, :]\n",
        "\n",
        "            # Create sample dictionary\n",
        "            sample = {'feature_matrix': feature_matrix,\n",
        "                      'midi_path': midi_path}\n",
        "\n",
        "            if self.transform:\n",
        "                sample = self.transform(sample)\n",
        "\n",
        "            return sample\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f'Failed to load {midi_path}. Error: {e}')\n",
        "            return self.__getitem__(random.randint(0, self.__len__() - 1))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "1UWk6EaLxcry"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2.3 Dataset Instantiation and DataLoaders\n",
        "# Define paths to your data and metadata\n",
        "fma_small_dir = \"/content/drive/MyDrive/DL4AM_datasets/fma_small\"\n",
        "fma_metadata_file = \"/content/drive/MyDrive/DL4AM_datasets/fma_metadata/tracks.csv\"\n",
        "\n",
        "# Create dataset instance\n",
        "logger.info(\"Creating FMA dataset instance...\")\n",
        "fma_dataset = FMADataset(\n",
        "    root_dir=fma_small_dir,\n",
        "    metadata_file=fma_metadata_file,\n",
        "    transform=None,  # No transform needed since we handle tensor conversion in __getitem__\n",
        "    n_mels=128,\n",
        "    hop_length=512,\n",
        "    sample_rate=22050,\n",
        "    n_frames=860\n",
        ")\n",
        "\n",
        "# Create random train/validation split indices (80% train, 20% validation)\n",
        "train_indices, val_indices = fma_dataset.get_train_val_indices(val_size=0.2, random_state=42)\n",
        "logger.info(f\"Train set: {len(train_indices)} samples, Validation set: {len(val_indices)} samples\")\n",
        "\n",
        "# Create data loaders for CVAE model\n",
        "cvae_train_loader = DataLoader(\n",
        "    dataset=fma_dataset,\n",
        "    batch_size=4,\n",
        "    sampler=SubsetRandomSampler(train_indices),\n",
        "    num_workers=4,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "cvae_val_loader = DataLoader(\n",
        "    dataset=fma_dataset,\n",
        "    batch_size=4,\n",
        "    sampler=SubsetRandomSampler(val_indices),\n",
        "    num_workers=4,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "# Create data loaders for BiLSTM model (using the same dataset and split)\n",
        "bilstm_train_loader = DataLoader(\n",
        "    dataset=fma_dataset,\n",
        "    batch_size=4,\n",
        "    sampler=SubsetRandomSampler(train_indices),\n",
        "    num_workers=4,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "bilstm_val_loader = DataLoader(\n",
        "    dataset=fma_dataset,\n",
        "    batch_size=4,\n",
        "    sampler=SubsetRandomSampler(val_indices),\n",
        "    num_workers=4,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "# Test the data loader\n",
        "logger.info(\"Testing data loader...\")\n",
        "try:\n",
        "    sample_batch = next(iter(cvae_train_loader))\n",
        "    logger.info(f\"Sample batch spectrogram shape: {sample_batch['spectrogram'].shape}\")\n",
        "    logger.info(f\"Sample batch class labels: {sample_batch['class_label'][:5]}\")  # Show first 5 labels\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error testing data loader: {e}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "eyS5FEkW5mba"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3 Models"
      ],
      "metadata": {
        "id": "96pqZKE70Cqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3.1 CVAE Source Separator\n",
        "\n",
        "\n",
        "#helper function\n",
        "# def compute_conv_shape(h, w, layers=3, kernel=3, stride=2, padding=1):\n",
        "#     for _ in range(layers):\n",
        "#         h = (h + 2 * padding - kernel) // stride + 1\n",
        "#         w = (w + 2 * padding - kernel) // stride + 1\n",
        "#     return h, w\n",
        "\n",
        "def compute_conv_output_shape(input_dim, n_frames, layers=3, kernel_size=3, stride=2, padding=1):\n",
        "    h, w = input_dim, n_frames\n",
        "    for _ in range(layers):\n",
        "        h = (h + 2*padding - kernel_size) // stride + 1\n",
        "        w = (w + 2*padding - kernel_size) // stride + 1\n",
        "    return h, w\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, latent_dim, n_frames=860):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
        "        h, w = compute_conv_output_shape(input_dim, n_frames)\n",
        "        self.flatten_size = 128 * h * w\n",
        "        # Calculate flattened size without creating tensors\n",
        "\n",
        "\n",
        "        self.fc1 = nn.Linear(self.flatten_size, hidden_dim)\n",
        "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = x.view(x.shape[0], -1)  # Flatten\n",
        "        x = F.relu(self.fc1(x))\n",
        "        mu = self.fc_mu(x)\n",
        "        log_var = self.fc_logvar(x)\n",
        "        return mu, log_var\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, output_dim, n_frames=860):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.fc1 = nn.Linear(latent_dim, hidden_dim)\n",
        "        self.n_frames = n_frames\n",
        "\n",
        "        # Calculate the dimensions after encoder's conv layers\n",
        "        h, w = compute_conv_output_shape(output_dim, n_frames)\n",
        "        self.encoded_channels = 128\n",
        "        self.encoded_height = h\n",
        "        self.encoded_width = w\n",
        "        self.flatten_size = self.encoded_channels * self.encoded_height * self.encoded_width\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        self.fc2 = nn.Linear(hidden_dim, self.flatten_size)\n",
        "\n",
        "        # Transposed convolutions for upsampling\n",
        "        self.deconv1 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "        self.deconv2 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "        self.deconv3 = nn.ConvTranspose2d(32, 1, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = x.view(x.shape[0], self.encoded_channels, self.encoded_height, self.encoded_width)\n",
        "        x = F.relu(self.deconv1(x))\n",
        "        x = F.relu(self.deconv2(x))\n",
        "        x = self.deconv3(x)\n",
        "\n",
        "        # Ensure the output has the exact same dimensions as the input\n",
        "        # This handles any dimension mismatch from transposed convolutions\n",
        "        x = F.interpolate(x, size=(128, self.n_frames), mode='bilinear', align_corners=False)\n",
        "\n",
        "        reconstruction = torch.sigmoid(x)\n",
        "        return reconstruction\n",
        "\n",
        "\n",
        "class CVAE(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, latent_dim, n_frames=860):\n",
        "        super(CVAE, self).__init__()\n",
        "        h, w = compute_conv_output_shape(input_dim, n_frames)\n",
        "        self.flatten_size = 128 * h * w\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder_conv = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        # Calculate flattened size\n",
        "        h, w = compute_conv_output_shape(input_dim, n_frames)\n",
        "        self.flatten_size = 128 * h * w\n",
        "\n",
        "\n",
        "\n",
        "        self.fc_mu = nn.Linear(self.flatten_size, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(self.flatten_size, latent_dim)\n",
        "\n",
        "        # Decoder\n",
        "        self.fc_decoder = nn.Linear(latent_dim, self.flatten_size)\n",
        "\n",
        "        self.decoder_conv = nn.Sequential(\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 1, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.n_frames = n_frames\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = self.encoder_conv(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        mu = self.fc_mu(x)\n",
        "        log_var = self.fc_logvar(x)\n",
        "        return mu, log_var\n",
        "\n",
        "    def reparameterize(self, mu, log_var):\n",
        "        std = torch.exp(0.5 * log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "        z = mu + eps * std\n",
        "        return z\n",
        "\n",
        "    def decode(self, z):\n",
        "        x = self.fc_decoder(z)\n",
        "        h, w = compute_conv_output_shape(self.input_dim, self.n_frames)\n",
        "        x = x.view(x.size(0), 128, h, w)\n",
        "        x = self.decoder_conv(x)\n",
        "        # Ensure output dimensions match input\n",
        "        if x.shape[2:] != (self.input_dim, self.n_frames):\n",
        "            x = F.interpolate(x, size=(self.input_dim, self.n_frames), mode='bilinear', align_corners=False)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, log_var = self.encode(x)\n",
        "        z = self.reparameterize(mu, log_var)\n",
        "        recon_x = self.decode(z)\n",
        "        return recon_x, mu, log_var"
      ],
      "metadata": {
        "id": "jJ2aq55ozujL",
        "cellView": "form"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3.2 BiLSTM Loop Candidacy Classifier\n",
        "\n",
        "class BiLSTMLoopClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Bidirectional LSTM-based binary classifier for predicting loop candidacy.\n",
        "    Takes mel spectrograms as input and outputs binary classification.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim=128, hidden_dim=256, num_layers=2, dropout=0.3):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_dim (int): Number of input features (mel bins)\n",
        "            hidden_dim (int): Hidden dimension of the LSTM\n",
        "            num_layers (int): Number of LSTM layers\n",
        "            dropout (float): Dropout probability (applied between LSTM layers and before final classification)\n",
        "        \"\"\"\n",
        "        super(BiLSTMLoopClassifier, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout_prob = dropout\n",
        "\n",
        "        # Bidirectional LSTM layers\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=dropout if num_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        # Feature extraction from the LSTM output\n",
        "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)  # *2 for bidirectional\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Final classification layer\n",
        "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the BiLSTM classifier.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape [batch_size, 1, mel_bins, time_frames]\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Probability score for loop candidacy\n",
        "        \"\"\"\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        # Reshape from [batch_size, 1, mel_bins, time_frames] to [batch_size, time_frames, mel_bins]\n",
        "        # by removing channel dimension and transposing\n",
        "        x = x.squeeze(1).transpose(1, 2)\n",
        "\n",
        "        # Pass through LSTM\n",
        "        lstm_out, _ = self.lstm(x)  # [batch_size, time_frames, hidden_dim*2]\n",
        "\n",
        "        # Extract features from the last time step\n",
        "        # Alternative: Use attention mechanism over all time steps\n",
        "        last_hidden = lstm_out[:, -1, :]  # [batch_size, hidden_dim*2]\n",
        "\n",
        "        # Feature processing\n",
        "        x = F.relu(self.fc1(last_hidden))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Final classification\n",
        "        logits = self.fc2(x)\n",
        "\n",
        "        # Return sigmoid output for binary classification\n",
        "        # return torch.sigmoid(logits).squeeze(-1)\n",
        "        # Return raw logits for BCEWithLogitsLoss\n",
        "        return logits.squeeze(-1)\n",
        "\n",
        "class LSTMWithAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Variation of the loop classifier that incorporates an attention mechanism\n",
        "    over the LSTM outputs to better focus on parts of the sequence that\n",
        "    are most relevant for loop candidacy prediction.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim=128, hidden_dim=256, num_layers=2, dropout=0.3):\n",
        "        super(LSTMWithAttention, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout_prob = dropout\n",
        "\n",
        "        # BiLSTM layers\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=dropout if num_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        # Attention mechanism\n",
        "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
        "\n",
        "        # Output layers\n",
        "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Reshape from [batch_size, 1, mel_bins, time_frames] to [batch_size, time_frames, mel_bins]\n",
        "        x = x.squeeze(1).transpose(1, 2)\n",
        "\n",
        "        # Pass through BiLSTM\n",
        "        lstm_out, _ = self.lstm(x)  # [batch_size, time_frames, hidden_dim*2]\n",
        "\n",
        "        # Calculate attention weights\n",
        "        attention_weights = F.softmax(self.attention(lstm_out), dim=1)\n",
        "\n",
        "        # Apply attention weights to get context vector\n",
        "        context = torch.sum(attention_weights * lstm_out, dim=1)\n",
        "\n",
        "        # Process context vector\n",
        "        x = F.relu(self.fc1(context))\n",
        "        x = self.dropout(x)\n",
        "        logits = self.fc2(x)\n",
        "\n",
        "        # Return sigmoid output\n",
        "        # return torch.sigmoid(logits).squeeze(-1)\n",
        "        # Return raw logits for BCEWithLogitsLoss\n",
        "        return logits.squeeze(-1)\n",
        "\n",
        "class RhythmicFeatureExtractor(nn.Module):\n",
        "    \"\"\"\n",
        "    Specialized convolutional module for extracting rhythmic features\n",
        "    from spectrograms before passing to BiLSTM. This can help detect\n",
        "    patterns that are important for loop candidacy.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels=1, out_channels=16):\n",
        "        super(RhythmicFeatureExtractor, self).__init__()\n",
        "\n",
        "        # Horizontal convolutions to capture time patterns\n",
        "        self.time_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=(1, 7), padding=(0, 3)),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=(1, 2))\n",
        "        )\n",
        "\n",
        "        # Vertical convolutions to capture frequency patterns\n",
        "        self.freq_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=(7, 1), padding=(3, 0)),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=(2, 1))\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply time and frequency convolutions\n",
        "        time_features = self.time_conv(x)  # Captures rhythm patterns\n",
        "        freq_features = self.freq_conv(x)  # Captures tonal patterns\n",
        "\n",
        "        # Concatenate features along the channel dimension\n",
        "        return torch.cat([time_features, freq_features], dim=1)\n",
        "\n",
        "class HybridLoopClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    A hybrid model combining convolutional feature extraction with BiLSTM\n",
        "    for loop candidacy classification. This architecture first extracts\n",
        "    rhythmic and tonal patterns from the spectrogram using CNNs and then\n",
        "    analyzes the temporal dependencies using a BiLSTM.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim=128, hidden_dim=256, num_layers=2, dropout=0.3):\n",
        "        super(HybridLoopClassifier, self).__init__()\n",
        "\n",
        "        # Feature extractor\n",
        "        self.feature_extractor = RhythmicFeatureExtractor(in_channels=1, out_channels=16)\n",
        "\n",
        "        # Adapting dimensions for LSTM\n",
        "        self.conv_adapt = nn.Conv2d(32, 1, kernel_size=1)\n",
        "\n",
        "        # BiLSTM for sequence modeling (input dim is halved due to pooling)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_dim // 2,  # Reduced by frequency pooling\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=dropout if num_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        # Output layers\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Extract rhythmic features with CNN\n",
        "        x = self.feature_extractor(x)  # [batch_size, 32, mel_bins/2, time_frames/2]\n",
        "\n",
        "        # Adapt channels for LSTM\n",
        "        x = self.conv_adapt(x)  # [batch_size, 1, mel_bins/2, time_frames/2]\n",
        "\n",
        "        # Reshape for LSTM: [batch_size, time_frames/2, mel_bins/2]\n",
        "        x = x.squeeze(1).transpose(1, 2)\n",
        "\n",
        "        # Process with BiLSTM\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "\n",
        "        # Global pooling across time\n",
        "        x = lstm_out.transpose(1, 2)  # [batch_size, hidden_dim*2, time_frames/2]\n",
        "        x = self.global_pool(x).squeeze(-1)  # [batch_size, hidden_dim*2]\n",
        "\n",
        "        # Final classification\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        logits = self.fc2(x)\n",
        "\n",
        "        # return torch.sigmoid(logits).squeeze(-1)\n",
        "\n",
        "        # Return raw logits for BCEWithLogitsLoss\n",
        "        return logits.squeeze(-1)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "2cb4IkoC0Rcv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4 Training Procedures"
      ],
      "metadata": {
        "id": "MdXdwV7n-k1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 4.1 CVAE Training Loop\n",
        "\n",
        "def loss_function(recon_x, x, mu, log_var, beta=0.001):\n",
        "    \"\"\"\n",
        "    Compute the CVAE loss function.\n",
        "\n",
        "    Args:\n",
        "        recon_x (torch.Tensor): The reconstructed spectrogram\n",
        "        x (torch.Tensor): The original spectrogram\n",
        "        mu (torch.Tensor): The mean of the latent Gaussian\n",
        "        log_var (torch.Tensor): The log variance of the latent Gaussian\n",
        "        beta (float): The weight for the KL divergence term\n",
        "\n",
        "    Returns:\n",
        "        tuple: (total_loss, reconstruction_loss, kl_divergence)\n",
        "    \"\"\"\n",
        "    # MSE loss for reconstruction (can be replaced with BCE if needed)\n",
        "    # reconstruction_loss = F.mse_loss(recon_x, x, reduction='sum')\n",
        "\n",
        "    # BCE loss for reconstruction (alternative)\n",
        "    reconstruction_loss = F.binary_cross_entropy(recon_x, x, reduction='mean')\n",
        "\n",
        "    # KL divergence: -0.5 * sum(1 + log_var - mu^2 - exp(log_var))\n",
        "    kl_divergence = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "\n",
        "    # Total loss is reconstruction loss plus weighted KL divergence\n",
        "    total_loss = reconstruction_loss + beta * kl_divergence\n",
        "\n",
        "    return total_loss, reconstruction_loss, kl_divergence\n",
        "\n",
        "def train_cvae(model, train_loader, val_loader=None, epochs=100, lr=1e-4,\n",
        "              beta=0.001, device='cuda', checkpoint_dir='./checkpoints',\n",
        "              checkpoint_interval=10, early_stopping_patience=10):\n",
        "    \"\"\"\n",
        "    Train the CVAE model.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The CVAE model\n",
        "        train_loader (DataLoader): DataLoader for training data\n",
        "        val_loader (DataLoader, optional): DataLoader for validation data\n",
        "        epochs (int): Number of training epochs\n",
        "        lr (float): Learning rate\n",
        "        beta (float): Weight for the KL divergence term\n",
        "        device (str): Device to train on ('cuda' or 'cpu')\n",
        "        checkpoint_dir (str): Directory to save model checkpoints\n",
        "        checkpoint_interval (int): Save model every n epochs\n",
        "        early_stopping_patience (int): Stop training if validation loss doesn't improve for n epochs\n",
        "\n",
        "    Returns:\n",
        "        nn.Module: The trained model\n",
        "        dict: Training history\n",
        "    \"\"\"\n",
        "\n",
        "    accumulation_steps = 8 #number of steps to accumulate gradients\n",
        "\n",
        "    # Create checkpoint directory if it doesn't exist\n",
        "    if not os.path.exists(checkpoint_dir):\n",
        "        os.makedirs(checkpoint_dir)\n",
        "\n",
        "    # Move model to device\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Initialize optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=5, verbose=True)\n",
        "\n",
        "    # Initialize training history\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'train_recon_loss': [],\n",
        "        'train_kl_loss': [],\n",
        "        'val_loss': [],\n",
        "        'val_recon_loss': [],\n",
        "        'val_kl_loss': []\n",
        "    }\n",
        "\n",
        "    # Early stopping variables\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    best_model_path = os.path.join(checkpoint_dir, 'cvae_best_model.pt')\n",
        "\n",
        "    logger.info(f\"Starting CVAE training for {epochs} epochs\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        start_time = time.time()\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        train_recon_loss = 0\n",
        "        train_kl_loss = 0\n",
        "\n",
        "        # Progress bar for training batches\n",
        "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\")\n",
        "\n",
        "        # Training loop over batches\n",
        "        for batch_idx, sample in enumerate(train_pbar):\n",
        "            # Get spectrogram from sample\n",
        "            data = sample['spectrogram'].to(device).float()\n",
        "\n",
        "            # Zero gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            recon_batch, mu, log_var = model(data)\n",
        "            loss, recon_loss, kl_loss = loss_function(recon_batch, data, mu, log_var, beta)\n",
        "\n",
        "            loss = loss / accumulation_steps\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            if (batch_idx + 1) % accumulation_steps == 0:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            print(f\"Recon shape: {recon_batch.shape}, Target shape: {data.shape}\")\n",
        "            print(f\"Recon min/max: {recon_batch.min().item()}/{recon_batch.max().item()}\")\n",
        "            print(f\"Target min/max: {data.min().item()}/{data.max().item()}\")\n",
        "\n",
        "            # Calculate loss\n",
        "            loss, recon_loss, kl_loss = loss_function(recon_batch, data, mu, log_var, beta)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Update weights\n",
        "            optimizer.step()\n",
        "\n",
        "            # Accumulate batch loss\n",
        "            train_loss += loss.item()\n",
        "            train_recon_loss += recon_loss.item()\n",
        "            train_kl_loss += kl_loss.item()\n",
        "\n",
        "            # Update progress bar\n",
        "            train_pbar.set_postfix({\n",
        "                'loss': loss.item() / len(data),\n",
        "                'recon_loss': recon_loss.item() / len(data),\n",
        "                'kl_loss': kl_loss.item() / len(data)\n",
        "            })\n",
        "\n",
        "        # Calculate average training loss for the epoch\n",
        "        avg_train_loss = train_loss / len(train_loader.dataset)\n",
        "        avg_train_recon_loss = train_recon_loss / len(train_loader.dataset)\n",
        "        avg_train_kl_loss = train_kl_loss / len(train_loader.dataset)\n",
        "\n",
        "        # Store training loss in history\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "        history['train_recon_loss'].append(avg_train_recon_loss)\n",
        "        history['train_kl_loss'].append(avg_train_kl_loss)\n",
        "\n",
        "        # Log training metrics\n",
        "        logger.info(f\"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f} \"\n",
        "                   f\"(Recon: {avg_train_recon_loss:.4f}, KL: {avg_train_kl_loss:.4f})\")\n",
        "\n",
        "        logger.info(f\"Epoch {epoch+1} completed in {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "        # Validation phase\n",
        "        if val_loader is not None:\n",
        "            model.eval()\n",
        "            val_loss = 0\n",
        "            val_recon_loss = 0\n",
        "            val_kl_loss = 0\n",
        "\n",
        "            # No gradient calculation during validation\n",
        "            with torch.no_grad():\n",
        "                # Progress bar for validation batches\n",
        "                val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\")\n",
        "\n",
        "                # Validation loop over batches\n",
        "                for batch_idx, sample in enumerate(val_pbar):\n",
        "                    # Get spectrogram from sample\n",
        "                    data = sample['spectrogram'].to(device).float()\n",
        "\n",
        "\n",
        "                    # Forward pass\n",
        "                    recon_batch, mu, log_var = model(data)\n",
        "\n",
        "                    # Calculate loss\n",
        "                    loss, recon_loss, kl_loss = loss_function(recon_batch, data, mu, log_var, beta)\n",
        "\n",
        "                    # Accumulate batch loss\n",
        "                    val_loss += loss.item()\n",
        "                    val_recon_loss += recon_loss.item()\n",
        "                    val_kl_loss += kl_loss.item()\n",
        "\n",
        "                    # Update progress bar\n",
        "                    val_pbar.set_postfix({\n",
        "                        'val_loss': loss.item() / len(data),\n",
        "                        'val_recon_loss': recon_loss.item() / len(data),\n",
        "                        'val_kl_loss': kl_loss.item() / len(data)\n",
        "                    })\n",
        "\n",
        "            # Calculate average validation loss for the epoch\n",
        "            avg_val_loss = val_loss / len(val_loader.dataset)\n",
        "            avg_val_recon_loss = val_recon_loss / len(val_loader.dataset)\n",
        "            avg_val_kl_loss = val_kl_loss / len(val_loader.dataset)\n",
        "\n",
        "            # Store validation loss in history\n",
        "            history['val_loss'].append(avg_val_loss)\n",
        "            history['val_recon_loss'].append(avg_val_recon_loss)\n",
        "            history['val_kl_loss'].append(avg_val_kl_loss)\n",
        "\n",
        "            # Log validation metrics\n",
        "            logger.info(f\"Epoch {epoch+1}/{epochs} - Val Loss: {avg_val_loss:.4f} \"\n",
        "                       f\"(Recon: {avg_val_recon_loss:.4f}, KL: {avg_val_kl_loss:.4f})\")\n",
        "\n",
        "            # Update learning rate based on validation loss\n",
        "            scheduler.step(avg_val_loss)\n",
        "\n",
        "            # Check for early stopping\n",
        "            if avg_val_loss < best_val_loss:\n",
        "                logger.info(f\"Validation loss improved from {best_val_loss:.4f} to {avg_val_loss:.4f}\")\n",
        "                best_val_loss = avg_val_loss\n",
        "                patience_counter = 0\n",
        "\n",
        "                # Save the best model\n",
        "                torch.save({\n",
        "                    'epoch': epoch,\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'loss': avg_val_loss,\n",
        "                }, best_model_path)\n",
        "                logger.info(f\"Saved best model to {best_model_path}\")\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                logger.info(f\"Validation loss did not improve. Patience: {patience_counter}/{early_stopping_patience}\")\n",
        "\n",
        "                if patience_counter >= early_stopping_patience:\n",
        "                    logger.info(f\"Early stopping triggered after {epoch+1} epochs\")\n",
        "                    break\n",
        "\n",
        "        # Save checkpoint periodically\n",
        "        if (epoch + 1) % checkpoint_interval == 0:\n",
        "            checkpoint_path = os.path.join(checkpoint_dir, f'cvae_epoch_{epoch+1}.pt')\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': avg_train_loss,\n",
        "            }, checkpoint_path)\n",
        "            logger.info(f\"Saved checkpoint to {checkpoint_path}\")\n",
        "\n",
        "    # Load the best model if validation was used\n",
        "    if val_loader is not None and os.path.exists(best_model_path):\n",
        "        logger.info(f\"Loading best model from {best_model_path}\")\n",
        "        checkpoint = torch.load(best_model_path)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    logger.info(\"CVAE training completed\")\n",
        "\n",
        "    return model, history\n",
        "\n",
        "\n",
        "def visualize_cvae_training(history):\n",
        "    \"\"\"\n",
        "    Visualize the training history of the CVAE model.\n",
        "\n",
        "    Args:\n",
        "        history (dict): Training history containing loss metrics\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # Plot total loss\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(history['train_loss'], label='Train')\n",
        "    if 'val_loss' in history and len(history['val_loss']) > 0:\n",
        "        plt.plot(history['val_loss'], label='Validation')\n",
        "    plt.title('Total Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot reconstruction loss\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.plot(history['train_recon_loss'], label='Train')\n",
        "    if 'val_recon_loss' in history and len(history['val_recon_loss']) > 0:\n",
        "        plt.plot(history['val_recon_loss'], label='Validation')\n",
        "    plt.title('Reconstruction Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot KL divergence\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.plot(history['train_kl_loss'], label='Train')\n",
        "    if 'val_kl_loss' in history and len(history['val_kl_loss']) > 0:\n",
        "        plt.plot(history['val_kl_loss'], label='Validation')\n",
        "    plt.title('KL Divergence')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def generate_samples(model, num_samples=5, latent_dim=32, device='cuda'):\n",
        "    \"\"\"\n",
        "    Generate samples from the trained CVAE model.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The trained CVAE model\n",
        "        num_samples (int): Number of samples to generate\n",
        "        latent_dim (int): Dimension of the latent space\n",
        "        device (str): Device to use ('cuda' or 'cpu')\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Generated samples\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Sample from the latent space\n",
        "        z = torch.randn(num_samples, latent_dim).to(device)\n",
        "\n",
        "        # Decode the latent samples\n",
        "        samples = model.decoder(z)\n",
        "\n",
        "    return samples\n",
        "\n",
        "\n",
        "def visualize_cvae_reconstructions(model, dataloader, num_examples=5, device='cuda'):\n",
        "    \"\"\"\n",
        "    Visualize original spectrograms and their reconstructions.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The trained CVAE model\n",
        "        dataloader (DataLoader): DataLoader containing the data\n",
        "        num_examples (int): Number of examples to visualize\n",
        "        device (str): Device to use ('cuda' or 'cpu')\n",
        "    \"\"\"\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Get examples from dataloader\n",
        "    examples = []\n",
        "    for batch_idx, sample in enumerate(dataloader):\n",
        "        if batch_idx >= num_examples:\n",
        "            break\n",
        "        examples.append(sample['spectrogram'])\n",
        "\n",
        "    # Concatenate examples\n",
        "    examples = torch.cat(examples[:num_examples])\n",
        "    examples = examples.to(device)\n",
        "\n",
        "    # Generate reconstructions\n",
        "    with torch.no_grad():\n",
        "        reconstructions, _, _ = model(examples)\n",
        "\n",
        "    # Move tensors to CPU for visualization\n",
        "    examples = examples.cpu()\n",
        "    reconstructions = reconstructions.cpu()\n",
        "\n",
        "    # Visualize originals and reconstructions\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    for i in range(num_examples):\n",
        "        # Original spectrogram\n",
        "        plt.subplot(2, num_examples, i + 1)\n",
        "        plt.imshow(examples[i].squeeze(), cmap='viridis', origin='lower', aspect='auto')\n",
        "        plt.title(f\"Original {i+1}\")\n",
        "        plt.colorbar()\n",
        "\n",
        "        # Reconstructed spectrogram\n",
        "        plt.subplot(2, num_examples, num_examples + i + 1)\n",
        "        plt.imshow(reconstructions[i].squeeze(), cmap='viridis', origin='lower', aspect='auto')\n",
        "        plt.title(f\"Reconstructed {i+1}\")\n",
        "        plt.colorbar()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Enj7FzpJ-lyb",
        "cellView": "form"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 4.2 BiLSTM Training Loop\n",
        "\n",
        "def train_bilstm(model, train_loader, val_loader=None, epochs=100, lr=1e-3,\n",
        "                device='cuda', checkpoint_dir='./checkpoints',\n",
        "                checkpoint_interval=10, early_stopping_patience=10):\n",
        "    \"\"\"\n",
        "    Train the BiLSTM model.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The BiLSTM model\n",
        "        train_loader (DataLoader): DataLoader for training data\n",
        "        val_loader (DataLoader, optional): DataLoader for validation data\n",
        "        epochs (int): Number of training epochs\n",
        "        lr (float): Learning rate\n",
        "        device (str): Device to train on ('cuda' or 'cpu')\n",
        "        checkpoint_dir (str): Directory to save model checkpoints\n",
        "        checkpoint_interval (int): Save model every n epochs\n",
        "        early_stopping_patience (int): Stop training if validation metric doesn't improve for n epochs\n",
        "\n",
        "    Returns:\n",
        "        nn.Module: The trained model\n",
        "        dict: Training history\n",
        "    \"\"\"\n",
        "    # Create checkpoint directory if it doesn't exist\n",
        "    if not os.path.exists(checkpoint_dir):\n",
        "        os.makedirs(checkpoint_dir)\n",
        "\n",
        "    # Move model to device\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Initialize optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = ReduceLROnPlateau(optimizer, 'max', factor=0.5, patience=5, verbose=True)\n",
        "\n",
        "    # Initialize training history\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'train_acc': [],\n",
        "        'train_precision': [],\n",
        "        'train_recall': [],\n",
        "        'train_f1': [],\n",
        "        'val_loss': [],\n",
        "        'val_acc': [],\n",
        "        'val_precision': [],\n",
        "        'val_recall': [],\n",
        "        'val_f1': []\n",
        "    }\n",
        "\n",
        "    # Initialize early stopping\n",
        "    best_val_f1 = 0.0\n",
        "    patience_counter = 0\n",
        "    best_model_path = os.path.join(checkpoint_dir, 'bilstm_best_model.pt')\n",
        "\n",
        "    logger.info(f\"Starting BiLSTM training for {epochs} epochs\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Training loop\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        y_true_train = []\n",
        "        y_pred_train = []\n",
        "\n",
        "        # Progress bar for training\n",
        "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\")\n",
        "\n",
        "        for batch_idx, sample in enumerate(train_pbar):\n",
        "            # Get spectrogram and labels from sample\n",
        "            spec = sample['spectrogram'].to(device).float()\n",
        "            labels = sample['class_label'].to(device)\n",
        "\n",
        "            # Zero gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            logits = model(spec)\n",
        "            loss = F.binary_cross_entropy_with_logits(logits, labels.float())\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Collect true and predicted labels\n",
        "            y_true_train.extend(labels.tolist())\n",
        "            y_pred_train.extend((torch.sigmoid(logits) > 0.5).long().tolist())\n",
        "\n",
        "            train_loss += loss.item() * len(labels)\n",
        "\n",
        "            # Update progress bar\n",
        "            train_pbar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "        # Calculate training metrics\n",
        "        train_loss /= len(train_loader.dataset)\n",
        "        train_acc = accuracy_score(y_true_train, y_pred_train)\n",
        "        train_precision = precision_score(y_true_train, y_pred_train)\n",
        "        train_recall = recall_score(y_true_train, y_pred_train)\n",
        "        train_f1 = f1_score(y_true_train, y_pred_train)\n",
        "\n",
        "        # Store training metrics\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['train_precision'].append(train_precision)\n",
        "        history['train_recall'].append(train_recall)\n",
        "        history['train_f1'].append(train_f1)\n",
        "\n",
        "        # Log training metrics\n",
        "        logger.info(f\"Epoch {epoch+1}/{epochs} - \"\n",
        "                    f\"Train Loss: {train_loss:.4f}, \"\n",
        "                    f\"Acc: {train_acc:.4f}, \"\n",
        "                    f\"Precision: {train_precision:.4f}, \"\n",
        "                    f\"Recall: {train_recall:.4f}, \"\n",
        "                    f\"F1: {train_f1:.4f}\")\n",
        "\n",
        "        logger.info(f\"Epoch {epoch+1} train completed in {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "        # Validation loop\n",
        "        if val_loader is not None:\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "            y_true_val = []\n",
        "            y_pred_val = []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\")\n",
        "\n",
        "                for batch_idx, sample in enumerate(val_pbar):\n",
        "                    spec = sample['spectrogram'].to(device).float()\n",
        "                    labels = sample['class_label'].to(device)\n",
        "\n",
        "                    logits = model(spec)\n",
        "                    loss = F.binary_cross_entropy_with_logits(logits, labels.float())\n",
        "\n",
        "                    y_true_val.extend(labels.tolist())\n",
        "                    y_pred_val.extend((torch.sigmoid(logits) > 0.5).long().tolist())\n",
        "\n",
        "                    val_loss += loss.item() * len(labels)\n",
        "\n",
        "                    val_pbar.set_postfix({'val_loss': loss.item()})\n",
        "\n",
        "            # Calculate validation metrics\n",
        "            val_loss /= len(val_loader.dataset)\n",
        "            val_acc = accuracy_score(y_true_val, y_pred_val)\n",
        "            val_precision = precision_score(y_true_val, y_pred_val)\n",
        "            val_recall = recall_score(y_true_val, y_pred_val)\n",
        "            val_f1 = f1_score(y_true_val, y_pred_val)\n",
        "\n",
        "            # Store validation metrics\n",
        "            history['val_loss'].append(val_loss)\n",
        "            history['val_acc'].append(val_acc)\n",
        "            history['val_precision'].append(val_precision)\n",
        "            history['val_recall'].append(val_recall)\n",
        "            history['val_f1'].append(val_f1)\n",
        "\n",
        "            # Log validation metrics\n",
        "            logger.info(f\"Epoch {epoch+1}/{epochs} - \"\n",
        "                        f\"Val Loss: {val_loss:.4f}, \"\n",
        "                        f\"Acc: {val_acc:.4f}, \"\n",
        "                        f\"Precision: {val_precision:.4f}, \"\n",
        "                        f\"Recall: {val_recall:.4f}, \"\n",
        "                        f\"F1: {val_f1:.4f}\")\n",
        "\n",
        "            # Update learning rate based on F1\n",
        "            scheduler.step(val_f1)\n",
        "\n",
        "            # Check early stopping\n",
        "            if val_f1 > best_val_f1:\n",
        "                logger.info(f\"F1 improved from {best_val_f1:.4f} to {val_f1:.4f}\")\n",
        "                best_val_f1 = val_f1\n",
        "                patience_counter = 0\n",
        "                torch.save(model.state_dict(), best_model_path)\n",
        "                logger.info(f\"Best model saved to {best_model_path}\")\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                logger.info(f\"F1 did not improve. Patience: {patience_counter}/{early_stopping_patience}\")\n",
        "\n",
        "                if patience_counter == early_stopping_patience:\n",
        "                    logger.info(\"Early stopping triggered\")\n",
        "                    break\n",
        "\n",
        "        # Periodic checkpoints\n",
        "        if (epoch + 1) % checkpoint_interval == 0:\n",
        "            checkpoint_path = os.path.join(checkpoint_dir, f'bilstm_epoch_{epoch+1}.pt')\n",
        "            torch.save(model.state_dict(), checkpoint_path)\n",
        "            logger.info(f\"Checkpoint saved to {checkpoint_path}\")\n",
        "\n",
        "    # Load best model\n",
        "    if val_loader is not None:\n",
        "        model.load_state_dict(torch.load(best_model_path))\n",
        "        logger.info(f\"Best model loaded from {best_model_path}\")\n",
        "\n",
        "    logger.info(\"BiLSTM training completed\")\n",
        "\n",
        "    return model, history\n",
        "\n",
        "\n",
        "def visualize_bilstm_training(history):\n",
        "    \"\"\"Visualization of BiLSTM training history.\"\"\"\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(20, 10))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    # Plot losses\n",
        "    axes[0].plot(history['train_loss'], label='Train Loss')\n",
        "    if 'val_loss' in history:\n",
        "        axes[0].plot(history['val_loss'], label='Validation Loss')\n",
        "    axes[0].set_title(\"Loss\")\n",
        "    axes[0].set_xlabel(\"Epoch\")\n",
        "    axes[0].legend()\n",
        "\n",
        "    metric_names = [\"Accuracy\", \"Precision\", \"Recall\", \"F1\"]\n",
        "\n",
        "    # Plot other metrics\n",
        "    for i, name in enumerate(metric_names, start=1):\n",
        "        train_key = f\"train_{name.lower()}\"\n",
        "        val_key = f\"val_{name.lower()}\"\n",
        "\n",
        "        axes[i].plot(history[train_key], label=f'Train {name}')\n",
        "        if val_key in history:\n",
        "            axes[i].plot(history[val_key], label=f'Validation {name}')\n",
        "        axes[i].set_title(name)\n",
        "        axes[i].set_xlabel(\"Epoch\")\n",
        "        axes[i].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "QnKhBQxxu9G1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
      ],
      "metadata": {
        "id": "BNMHtOZYxNFz"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# # Define hyperparameter search space\n",
        "# model = CVAE(\n",
        "#     input_dim=128,  # Number of mel bands\n",
        "#     hidden_dim=128,\n",
        "#     latent_dim=cvae_p['latent_dim'],\n",
        "#     n_frames=860    # Number of frames in your spectrograms\n",
        "# )\n",
        "\n",
        "# print(torch.cuda.is_available())\n",
        "# print(torch.cuda.device_count())\n",
        "# # print(model.state_dict())\n",
        "# for name, param in model.named_parameters():\n",
        "#     print(f\"Parameter: {name}, Device: {param.device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "3LtxQ_kkzA_w",
        "outputId": "d73b964f-5cea-409e-bf3d-4c60606bbbad"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'cvae_p' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-002f8fbd3ef1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Number of mel bands\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mhidden_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mlatent_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcvae_p\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'latent_dim'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mn_frames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m860\u001b[0m    \u001b[0;31m# Number of frames in your spectrograms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'cvae_p' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # Forces PyTorch to give accurate stack traces\n",
        "\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# print(f\"Device: {device}\")\n",
        "\n",
        "# cvae_model = CVAE(input_dim=128, hidden_dim=128, latent_dim=32, n_frames=860)\n",
        "\n",
        "# print(\"Model created. Moving to device now...\")\n",
        "# model = model.to(device)\n",
        "# print(\"Moved model to device.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "J1guuVfgKC9j",
        "outputId": "e99de5f9-4c00-46d8-fc9d-767fe74508c0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Model created. Moving to device now...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-482a4041e9ca>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model created. Moving to device now...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Moved model to device.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Model size estimate calculations\n",
        "\n",
        "import torch\n",
        "\n",
        "# Calculate the model size\n",
        "total_params = sum(p.numel() for p in cvae_model.parameters())\n",
        "model_size_bytes = total_params * 4\n",
        "model_size_mib = model_size_bytes / (1024 ** 2)\n",
        "\n",
        "# Estimate the activation size (assuming activations are the same size as parameters)\n",
        "activation_size_bytes = model_size_bytes\n",
        "\n",
        "# Estimate the gradient size (same size as parameters)\n",
        "gradient_size_bytes = model_size_bytes\n",
        "\n",
        "# Specify the batch size\n",
        "batch_size = 4  # Replace with your actual batch size\n",
        "\n",
        "# Calculate the total memory required\n",
        "total_memory_bytes = (model_size_bytes + activation_size_bytes + gradient_size_bytes) * batch_size\n",
        "total_memory_mib = total_memory_bytes / (1024 ** 2)\n",
        "\n",
        "print(f\"Estimated model size: {model_size_mib:.2f} MiB\")\n",
        "print(f\"Estimated total memory required: {total_memory_mib:.2f} MiB\")\n",
        "\n",
        "total_params = sum(p.numel() for p in cvae_model.parameters())\n",
        "print(f\"Total number of parameters: {total_params:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xz8xr7Y73eeQ",
        "outputId": "71c32868-17a9-454f-d85a-c4445a0c497e",
        "cellView": "form"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated model size: 82.55 MiB\n",
            "Estimated total memory required: 990.61 MiB\n",
            "Total number of parameters: 21,640,129\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title encoder test for resolving cuda issues\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Sanity check: create and move a simple Conv2d layer\n",
        "try:\n",
        "    test_conv = nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1)\n",
        "    test_conv.to(device)\n",
        "    print(\"✅ Conv2d test passed\")\n",
        "except Exception as e:\n",
        "    print(\"❌ Conv2d failed:\", e)\n",
        "\n",
        "# Now try only your encoder\n",
        "try:\n",
        "    encoder = Encoder(input_dim=128, hidden_dim=128, latent_dim=32, n_frames=860)\n",
        "    print(\"✅ Encoder init complete\")\n",
        "    encoder.to(device)\n",
        "    print(\"✅ Encoder moved to device\")\n",
        "except Exception as e:\n",
        "    print(\"❌ Encoder failed:\", e)\n",
        "\n",
        "# Now try decoder\n",
        "try:\n",
        "    decoder = Decoder(latent_dim=32, hidden_dim=128, output_dim=128, n_frames=860)\n",
        "    print(\"✅ Decoder init complete\")\n",
        "    decoder.to(device)\n",
        "    print(\"✅ Decoder moved to device\")\n",
        "except Exception as e:\n",
        "    print(\"❌ Decoder failed:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlzL6HY5OWMi",
        "outputId": "f341c131-8ff5-456c-f101-3fb40839759b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "✅ Conv2d test passed\n",
            "✅ Encoder init complete\n",
            "✅ Encoder moved to device\n",
            "✅ Decoder init complete\n",
            "✅ Decoder moved to device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 4.3 Train Models\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# === Hyperparameter Tuning ===\n",
        "# Define hyperparameter search space\n",
        "cvae_params = {\n",
        "    'latent_dim': [16, 32, 64],     # Dimensionality of the latent space\n",
        "    'lr': [1e-4, 1e-3],             # Learning rates to try\n",
        "    'beta': [0.001, 0.01, 0.1],     # Weighting for KL divergence in the loss\n",
        "}\n",
        "\n",
        "bilstm_params = {\n",
        "    'hidden_dim': [64, 128, 256],\n",
        "    'num_layers': [1, 2, 3],\n",
        "    'dropout': [0.1, 0.3, 0.5],\n",
        "    'lr': [1e-4, 1e-3, 1e-2],\n",
        "}\n",
        "\n",
        "\n",
        "# Perform grid search over hyperparameters\n",
        "best_cvae_params = None\n",
        "best_cvae_val_loss = float('inf')\n",
        "best_bilstm_params = None\n",
        "best_bilstm_val_f1 = 0.0\n",
        "\n",
        "for cvae_p in ParameterGrid(cvae_params):\n",
        "    for bilstm_p in ParameterGrid(bilstm_params):\n",
        "\n",
        "        logger.info(f\"Training with CVAE params: {cvae_p}, BiLSTM params: {bilstm_p}\")\n",
        "\n",
        "        # Create models with current hyperparameters\n",
        "        cvae_model = CVAE(input_dim=128, hidden_dim=128, latent_dim=cvae_p['latent_dim'])\n",
        "        # bilstm_model = BiLSTMLoopClassifier(input_dim=n_mels, hidden_dim=bilstm_p['hidden_dim'],\n",
        "                                            # num_layers=bilstm_p['num_layers'], dropout=bilstm_p['dropout'])\n",
        "\n",
        "        # === Train CVAE ===\n",
        "    try:\n",
        "        trained_cvae, cvae_history = train_cvae(\n",
        "            model=cvae_model,\n",
        "            train_loader=cvae_train_loader,\n",
        "            val_loader=cvae_val_loader,\n",
        "            epochs=100,\n",
        "            lr=cvae_p['lr'],\n",
        "            beta=cvae_p['beta'],\n",
        "            device=device,\n",
        "            checkpoint_dir=\"./checkpoints/cvae\"\n",
        "        )\n",
        "    except RuntimeError as e:\n",
        "        print(f\"Final CVAE training failed: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        trained_cvae, cvae_history = None, None\n",
        "        continue  # skip to next hyperparameter combo if training fails\n",
        "\n",
        "    # ✅ Only check best after successful training\n",
        "    if cvae_history:\n",
        "        min_val_loss = min(cvae_history['val_loss'])\n",
        "        if min_val_loss < best_cvae_val_loss:\n",
        "            best_cvae_val_loss = min_val_loss\n",
        "            best_cvae_params = cvae_p\n",
        "            logger.info(f\"New best CVAE with val loss {min_val_loss:.4f}\")\n",
        "            torch.save(trained_cvae.state_dict(), \"./checkpoints/cvae/best_cvae.pt\")\n",
        "\n",
        "\n",
        "        # # === Train BiLSTM ===\n",
        "        # trained_bilstm, bilstm_history = train_bilstm(\n",
        "        #     model=bilstm_model,\n",
        "        #     train_loader=bilstm_train_loader,\n",
        "        #     val_loader=bilstm_val_loader,\n",
        "        #     epochs=50,\n",
        "        #     lr=bilstm_p['lr'],\n",
        "        #     device=device,\n",
        "        #     checkpoint_dir=\"./checkpoints/bilstm\"\n",
        "        # )\n",
        "\n",
        "        # # Check if best BiLSTM so far\n",
        "        # max_val_f1 = max(bilstm_history['val_f1'])\n",
        "        # if max_val_f1 > best_bilstm_val_f1:\n",
        "        #     best_bilstm_val_f1 = max_val_f1\n",
        "        #     best_bilstm_params = bilstm_p\n",
        "        #     logger.info(f\"New best BiLSTM with val F1 {max_val_f1:.4f}\")\n",
        "        #     torch.save(trained_bilstm.state_dict(), \"./checkpoints/bilstm/best_bilstm.pt\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# === TEMP: Manually train CVAE with default params to verify it works ===\n",
        "\n",
        "manual_cvae_params = {\n",
        "    'latent_dim': 32,\n",
        "    'lr': 1e-4,\n",
        "    'beta': 0.001,\n",
        "}\n",
        "\n",
        "logger.info(\"Training CVAE manually with default hyperparameters to initialize best_cvae_params.\")\n",
        "cvae_model = CVAE(input_dim=n_mels, hidden_dim=128, latent_dim=manual_cvae_params['latent_dim'])\n",
        "trained_cvae, cvae_history = train_cvae(\n",
        "    model=cvae_model,\n",
        "    train_loader=cvae_train_loader,\n",
        "    val_loader=cvae_val_loader,\n",
        "    epochs=100,\n",
        "    lr=manual_cvae_params['lr'],\n",
        "    beta=manual_cvae_params['beta'],\n",
        "    device=device,\n",
        "    checkpoint_dir=\"./checkpoints/cvae\"\n",
        ")\n",
        "\n",
        "# Save these params as \"best\" for now so the rest of the code works\n",
        "best_cvae_params = manual_cvae_params\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # === Train Final Models with Best Hyperparameters ===\n",
        "# if best_cvae_params is None:\n",
        "#     raise RuntimeError(\"No successful CVAE training runs. Cannot proceed with final training.\")\n",
        "# logger.info(f\"Training final CVAE with params: {best_cvae_params}\")\n",
        "\n",
        "\n",
        "# cvae_model = CVAE(input_dim=n_mels, hidden_dim=128, latent_dim=best_cvae_params['latent_dim'])\n",
        "# trained_cvae, cvae_history = train_cvae(\n",
        "#     model=cvae_model,\n",
        "#     train_loader=cvae_train_loader,\n",
        "#     val_loader=cvae_val_loader,\n",
        "#     epochs=100,\n",
        "#     lr=best_cvae_params['lr'],\n",
        "#     beta=best_cvae_params['beta'],\n",
        "#     device=device,\n",
        "#     checkpoint_dir=\"./checkpoints/cvae\"\n",
        "# )\n",
        "\n",
        "# logger.info(f\"Training final BiLSTM with params: {best_bilstm_params}\")\n",
        "# bilstm_model = BiLSTMLoopClassifier(\n",
        "#     input_dim=n_mels, hidden_dim=best_bilstm_params['hidden_dim'],\n",
        "#     num_layers=best_bilstm_params['num_layers'], dropout=best_bilstm_params['dropout']\n",
        "# )\n",
        "# trained_bilstm, bilstm_history = train_bilstm(\n",
        "#     model=bilstm_model,\n",
        "#     train_loader=bilstm_train_loader,\n",
        "#     val_loader=bilstm_val_loader,\n",
        "#     epochs=100,\n",
        "#     lr=best_bilstm_params['lr'],\n",
        "#     device=device,\n",
        "#     checkpoint_dir=\"./checkpoints/bilstm\"\n",
        "# )\n",
        "\n",
        "# === Visualize Training Results ===\n",
        "visualize_cvae_training(cvae_history)\n",
        "visualize_cvae_reconstructions(trained_cvae, cvae_val_loader)\n",
        "\n",
        "# visualize_bilstm_training(bilstm_history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9T6aTrD4yR0D",
        "outputId": "00d59fc2-1432-4329-fa77-364dab0ef2d8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "Epoch 1/100 [Train]:   0%|          | 0/1600 [00:07<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-19-4423d364c031>\", line 39, in <cell line: 0>\n",
            "    trained_cvae, cvae_history = train_cvae(\n",
            "                                 ^^^^^^^^^^^\n",
            "  File \"<ipython-input-11-435ce48504fe>\", line 107, in train_cvae\n",
            "    loss, recon_loss, kl_loss = loss_function(recon_batch, data, mu, log_var, beta)\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-11-435ce48504fe>\", line 21, in loss_function\n",
            "    reconstruction_loss = F.binary_cross_entropy(recon_x, x, reduction='mean')\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\", line 3569, in binary_cross_entropy\n",
            "    return torch._C._nn.binary_cross_entropy(input, target, weight, reduction_enum)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: CUDA error: device-side assert triggered\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final CVAE training failed: CUDA error: device-side assert triggered\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "Final CVAE training failed: CUDA error: device-side assert triggered\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-19-4423d364c031>\", line 39, in <cell line: 0>\n",
            "    trained_cvae, cvae_history = train_cvae(\n",
            "                                 ^^^^^^^^^^^\n",
            "  File \"<ipython-input-11-435ce48504fe>\", line 61, in train_cvae\n",
            "    model = model.to(device)\n",
            "            ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1343, in to\n",
            "    return self._apply(convert)\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 903, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 903, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 930, in _apply\n",
            "    param_applied = fn(param)\n",
            "                    ^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1329, in convert\n",
            "    return t.to(\n",
            "           ^^^^^\n",
            "RuntimeError: CUDA error: device-side assert triggered\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final CVAE training failed: CUDA error: device-side assert triggered\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-19-4423d364c031>\", line 39, in <cell line: 0>\n",
            "    trained_cvae, cvae_history = train_cvae(\n",
            "                                 ^^^^^^^^^^^\n",
            "  File \"<ipython-input-11-435ce48504fe>\", line 61, in train_cvae\n",
            "    model = model.to(device)\n",
            "            ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1343, in to\n",
            "    return self._apply(convert)\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 903, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 903, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 930, in _apply\n",
            "    param_applied = fn(param)\n",
            "                    ^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1329, in convert\n",
            "    return t.to(\n",
            "           ^^^^^\n",
            "RuntimeError: CUDA error: device-side assert triggered\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final CVAE training failed: CUDA error: device-side assert triggered\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-19-4423d364c031>\", line 39, in <cell line: 0>\n",
            "    trained_cvae, cvae_history = train_cvae(\n",
            "                                 ^^^^^^^^^^^\n",
            "  File \"<ipython-input-11-435ce48504fe>\", line 61, in train_cvae\n",
            "    model = model.to(device)\n",
            "            ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1343, in to\n",
            "    return self._apply(convert)\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 903, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 903, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 930, in _apply\n",
            "    param_applied = fn(param)\n",
            "                    ^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1329, in convert\n",
            "    return t.to(\n",
            "           ^^^^^\n",
            "RuntimeError: CUDA error: device-side assert triggered\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-4423d364c031>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# Create models with current hyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mcvae_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCVAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcvae_p\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'latent_dim'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;31m# bilstm_model = BiLSTMLoopClassifier(input_dim=n_mels, hidden_dim=bilstm_p['hidden_dim'],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                                             \u001b[0;31m# num_layers=bilstm_p['num_layers'], dropout=bilstm_p['dropout'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-e937b9ace5ae>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dim, hidden_dim, latent_dim, n_frames)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_logvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_parameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bias\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mreset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;31m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# https://github.com/pytorch/pytorch/issues/57109\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkaiming_uniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0mfan_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_fan_in_and_fan_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/init.py\u001b[0m in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity, generator)\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstd\u001b[0m  \u001b[0;31m# Calculate uniform bounds from standard deviation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BAq-t2A322dK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}